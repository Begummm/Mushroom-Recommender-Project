{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Course\n",
    "----------------------------------------------------------\n",
    "# Mushroom Recommender Projet\n",
    "----------------------------------------------------------\n",
    "# By Begum SARIGUZEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom = pd.read_csv(\"mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom.drop(['gill-attachment'],axis=1,inplace=True)\n",
    "mushroom.drop(['veil-color'],axis=1,inplace=True)\n",
    "mushroom.drop(['ring-number'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom[mushroom.columns] = mushroom[mushroom.columns].astype('category')\n",
    "target = mushroom['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelen = LabelEncoder()\n",
    "mushroom_encode = mushroom[mushroom.columns].apply(lambda col: labelen.fit_transform(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom.drop(['veil-type'],axis=1,inplace=True)\n",
    "mushroom_encode.drop(['veil-type'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mushroom_encode['class']\n",
    "mushroom_encode.drop(['class'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=20, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(1, 2), learning_rate='constant',\n",
      "              learning_rate_init=0.05, max_iter=500, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1, 2),  #1 node, 2 nodes, 2 layers\n",
    "                    activation='relu',  #max(0,x) activation function\n",
    "                    solver='sgd',  # stochastic gradient descent\n",
    "                    alpha=0.0001, # L2 penalty                                    (sum of the alpha*square of parameters) which controls the size or height of the parameters(theta)\n",
    "                    batch_size=20, # size of minibatches\n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.05, \n",
    "                    max_iter=500, \n",
    "                    shuffle=True, # matter of ordering the data\n",
    "                    tol=0.00001,  #tolerance (epsilon in lecture 41)\n",
    "                    verbose=True) # having messages\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating small data consisted of odorless white spore point color mushrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9231905465288035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_odor = mushroom[mushroom['odor'].isin(['n'])]\n",
    "no_odor_w = no_odor[no_odor['spore-print-color'].isin(['w'])]\n",
    "(len(mushroom.index) - len(no_odor_w.index)) / len(mushroom.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_no_odor = no_odor_w['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_odor_encode = no_odor_w[no_odor_w.columns].apply(lambda col: labelen.fit_transform(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_no_odor = no_odor_encode['class']\n",
    "no_odor_encode.drop(['class'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\begum\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "column_names_no_odor = no_odor_w.columns\n",
    "no_odor_w.drop(['class'],axis=1,inplace=True)\n",
    "no_odor_preprocessed = pd.get_dummies(no_odor_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(no_odor_preprocessed,y_no_odor, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36751586\n",
      "Iteration 2, loss = 0.27566456\n",
      "Iteration 3, loss = 0.27490110\n",
      "Iteration 4, loss = 0.27415047\n",
      "Iteration 5, loss = 0.27404331\n",
      "Iteration 6, loss = 0.27436714\n",
      "Iteration 7, loss = 0.27474007\n",
      "Iteration 8, loss = 0.27419541\n",
      "Iteration 9, loss = 0.27447870\n",
      "Iteration 10, loss = 0.27441271\n",
      "Iteration 11, loss = 0.27447134\n",
      "Iteration 12, loss = 0.27425449\n",
      "Iteration 13, loss = 0.27457608\n",
      "Iteration 14, loss = 0.27472566\n",
      "Iteration 15, loss = 0.27440552\n",
      "Iteration 16, loss = 0.27444684\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=20, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(1, 2), learning_rate='constant',\n",
       "              learning_rate_init=0.05, max_iter=500, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (on training set) =  0.922\n",
      "Accuracy (on test set) =  0.9255\n"
     ]
    }
   ],
   "source": [
    "trainingScore = mlp.score(X_train,y_train)\n",
    "print(\"Accuracy (on training set) = \", round(trainingScore,4))\n",
    "\n",
    "testingScore = mlp.score(X_test,y_test)\n",
    "print(\"Accuracy (on test set) = \", round(testingScore,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom.drop(['class'],axis=1,inplace=True)\n",
    "mushroom_preprocessed = pd.get_dummies(mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(mushroom_preprocessed,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.14743516\n",
      "Iteration 2, loss = 0.05050139\n",
      "Iteration 3, loss = 0.05025434\n",
      "Iteration 4, loss = 0.05047573\n",
      "Iteration 5, loss = 0.05055218\n",
      "Iteration 6, loss = 0.05055952\n",
      "Iteration 7, loss = 0.05034329\n",
      "Iteration 8, loss = 0.05036758\n",
      "Iteration 9, loss = 0.05039472\n",
      "Iteration 10, loss = 0.05025470\n",
      "Iteration 11, loss = 0.05029274\n",
      "Iteration 12, loss = 0.05028405\n",
      "Iteration 13, loss = 0.05030947\n",
      "Iteration 14, loss = 0.05038889\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=20, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(1, 2), learning_rate='constant',\n",
       "              learning_rate_init=0.05, max_iter=500, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (on training set) =  0.99\n",
      "Accuracy (on test set) =  0.9873\n"
     ]
    }
   ],
   "source": [
    "trainingScore = mlp.score(X_train1,y_train1)\n",
    "print(\"Accuracy (on training set) = \", round(trainingScore,4))\n",
    "\n",
    "testingScore = mlp.score(X_test1,y_test1)\n",
    "print(\"Accuracy (on test set) = \", round(testingScore,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GridSearcCv to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'hidden_layer_sizes': [(5,5), (10,10), (15,15), (25,25)], \n",
    "   'learning_rate_init':[0.003, 0.01, 0.03, 0.1],\n",
    "   'alpha': [0.00001, 0.0001, 0.001, 0.01]}\n",
    " ]\n",
    "#print(param_grid)\n",
    "\n",
    "# Cross-validation grid-search\n",
    "scores = ['precision', 'recall']\n",
    "for score in scores:\n",
    "    clf2 = GridSearchCV( MLPClassifier(activation='relu', alpha=1e-07, batch_size=4, beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
    "       learning_rate_init=0.005, max_iter=500, momentum=0.8,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=11, shuffle=True,\n",
    "       solver='adam', tol=1e-05, validation_fraction=0.3, verbose=False,\n",
    "       warm_start=False), \n",
    "       param_grid, cv=3, scoring='%s_macro' % score)\n",
    "    \n",
    "    clf2.fit(X_train, y_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf2.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf2.cv_results_['mean_test_score']\n",
    "    stds = clf2.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf2.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "           % (mean, std * 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf2.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuilding the model considering the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(5,5),  #5 nodes, 2 layers\n",
    "                    activation='relu',  #max(0,x) activation function\n",
    "                    solver='sgd',  # stochastic gradient descent\n",
    "                    alpha=0.00001, # L2 penalty (sum of the alpha*square of parameters) which controls the size or height of the parameters(theta)\n",
    "                    batch_size=20, # size of minibatches\n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.003, \n",
    "                    max_iter=500, \n",
    "                    shuffle=True, # matter of ordering the data\n",
    "                    tol=0.00001,  #tolerance (epsilon in lecture 41)\n",
    "                    verbose=True) # having messages\n",
    "\n",
    "print(mlp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2.fit(X_train2=1, y_train1)\n",
    "trainingScore = mlp2.score(X_train1,y_train1)\n",
    "print(\"Accuracy (on training set) = \", round(trainingScore,4))\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "testingScore = mlp2.score(X_test1,y_test1)\n",
    "print(\"Accuracy (on test set) = \", round(testingScore,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
